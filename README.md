ğŸ“Œ Interview Partner â€“ AI-Powered Mock Interview System

An intelligent mock interview platform built using:

FastAPI Backend

React + Vite Frontend

Local Phi-3 Mini 4K Instruct (GGUF) as the LLM

Speech-to-Text voice input

Dynamic question generation

AI-generated interview summary + feedback

This app simulates a customized interview experience for different roles and provides structured feedback at the end.

ğŸš€ Features
ğŸ¤ Voice-Powered Interview

Real-time speech-to-text (browser microphone)

Auto-transcribed answers

ğŸ¤– AI-Driven Question Generator

Uses Phi-3 Mini GGUF model locally

Generates contextual follow-up questions

Avoids repeating previous questions

Fully offline â€” no API key needed

ğŸ§  Smart Feedback Report

At the end of the interview:

Overall feedback

Communication skill assessment

Technical understanding

Strengths

Areas for improvement

ğŸ—‚ Multiple User Personas

Select any role:

Software Engineer

Backend Developer

Data Scientist

React Developer

And moreâ€¦

ğŸ–¥ Full-stack Architecture

FastAPI backend exposes secure APIs

React frontend manages UI and session state

ğŸ“‚ Project Structure
interview-partner/
â”‚â”€â”€ backend/
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ load_llm.py
â”‚   â”‚   â””â”€â”€ interview_agent.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ Phi-3-mini-4k-instruct-q4.gguf
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ session_store.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env
â”‚
â”‚â”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   |
â”‚   â”‚   â””â”€â”€ components/...
â”‚   â”œâ”€â”€ index.html
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ README.md

âš™ï¸ Backend Setup (FastAPI + Local Phi Mini)
1ï¸âƒ£ Navigate to backend
cd backend

2ï¸âƒ£ Create virtual environment
python -m venv venv

3ï¸âƒ£ Activate environment

Windows

venv\Scripts\activate


macOS/Linux

source venv/bin/activate

4ï¸âƒ£ Install dependencies
pip install -r requirements.txt

5ï¸âƒ£ Place your LLM model (very important)

Model must exist here:

backend/models/Phi-3-mini-4k-instruct-q4.gguf

6ï¸âƒ£ Run backend
uvicorn main:app --reload


Backend runs at:

ğŸ‘‰ http://localhost:8000

ğŸ¨ Frontend Setup (React + Vite)
1ï¸âƒ£ Navigate to frontend
cd frontend

2ï¸âƒ£ Install dependencies
npm install

3ï¸âƒ£ Start frontend
npm run dev


Frontend runs at:

ğŸ‘‰ http://localhost:5173

ğŸ› System Architecture Overview
1. User Interface (React + Vite)

Handles role selection, question display, and voice input.

Uses Web Speech API for speech-to-text.

Communicates with backend using REST endpoints.

2. Backend Logic (FastAPI)

Routes:

Endpoint	Purpose
/start-interview	Creates session and sends first question
/next-question	AI generates next interview question
/submit-answer	Stores answer
/quit-interview	Generates AI feedback summary
3. LLM Engine (Phi-3 Mini GGUF)

Loaded locally using llama_cpp_python

All inference is done offline

Generates interview questions & end summary

4. Session Manager

Sessions stored in memory

Tracks conversation context

Prevents repeated questions

ğŸ§© Design Decisions
âœ” Local LLM over Cloud API

No cost

No rate limits

Completely private

Faster inference on CPU

âœ” FastAPI for Backend

Lightweight and async

Perfect for LLM-driven workloads

Easy to integrate with frontend

âœ” Custom Agent Instead of LangChain

Avoids overhead & complexity

Full control over prompts

Faster local execution

âœ” Voice-to-Text on Client Side

Zero backend load

No external API required

Works on all Chromium browsers


